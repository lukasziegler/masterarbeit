\section{Related Work}
\label{chapter:related-work}

	% Brief introduction
	The goal of the literature review was to find out how other researchers evaluated public displays and to develop an understanding of how these aspects can be addressed through surveys. The aim was to identify important aspects of public display deployments, from a researcher's as well form a practitioner's perspective. A short summary of the most relevant papers are described in this chapter.


	\subsection{Evaluation of Public Displays} % Previous evaluation

	Public display evaluation has already been addressed in literature. Alt et al. \cite{Alt2012HowToEvaluate} give an overview of study types, research paradigms, and evaluation methods used for evaluating public displays. M{\"u}ller et al. \cite{muller2014mirrortouch} present with MirrorTouch a follow-up evaluation and additionally extract metrics used for quantitative field studies. According to their findings, almost exclusively descriptive field studies are used in the area of public display evaluation. For a more in-depth introduction to public displays, the doctoral thesis from Alt \cite{alt2013thesis} gives an ideal overview.
	% General introduction to Evaluation 
	For a general recap of how to best design, evaluate, and report experiments, the book by Field and Hole \cite{field2003design} was used. Kirakowski \cite{kirakowski2000questionnaireFAQ} provides a useful introduction for practitioners of what to watch out for, covering the most important aspects for conducting surveys.




	\subsection{Exemplary Papers}

	A selection of papers, which have inspired us and which themselves have a good approach towards the evaluation of public displays, are amongst others: Overcoming Assumptions by Huang et al. \cite{huang2008overcoming}, Worlds of Information by Jacucci et al. \cite{jacucci2010worldsofinformation}, and Digifieds by Alt et al. \cite{alt2011digifieds}. In the following a short overview of these and other papers is given.


	% Worlds of Information - 2010
	Jacucci et al. provide with \cite{jacucci2010worldsofinformation}not only a superb overview of evaluation methods, but also the way they evaluate their results is exemplary. The evaluation is detailed and all questions asked in their questionnaires are stated. For evaluation they used video ethnography and a variety of questionnaires. Their focus in their paper lies on aspects of group use and user engagement, which they measure through questions adapted from Flow (GameFlow), Presence (MEC), and Intrinsic Motivation (IMI) questionnaires.

	% Overvcoming Assumptions (Huang) - 2008
	In \textit{Overcoming Assumptions} Huang et al. \cite{huang2008overcoming} analyzed how ``large ambient information displays in public settings'' (public displays) were deployed and tried to understand how such displays are used, in order to derive best practices and offer design implications. Recommendations given include elements like the position of the display, content, dynamics or how to best present the message you want to convey.
	% IM Here (Huang) - 2004
	In \textit{IM Here}, an older publication by Huang et al. \cite{Huang2004} observations, informal conversations and questionnaires are used for evaluation. The focus of their evaluation is social awareness and collaboration aspects. Questionnaires were conducted before and after the primary task.

	% UBI Hotspot (Ojala) - 2010 - #41
	Ojala et al. \cite{ojala2010ubi} present an evaluation of their long-term public display deployment in downtown Oulu, referred to as \textit{UBI Hotspot}. They conducted their surveys in-situ and evaluated data from a time span of eight months. Surveys were also already embedded directly onto the UBI hotspots. They aimed to analyze the usage and user acceptance rates for their setup with questions based on Nielsen's system acceptance model. Two general demographic questions (age and gender) were asked first, followed by a random selection of 8 statements from Nielsen's system acceptance model.


	% Digifieds (Alt) - 2011
	Alt et al. \cite{alt2011digifieds} created a digital noticeboard called \textit{Digifieds} and evaluated the platform using observations, interviews and a field trial. The Digifieds platform was deployed during the UbiChallenge 2011 to an urban environment in Finland and evaluated with the help of SUS questionnaires. The questionnaire accompanied the field trial and was structured as follows: after the users got a brief introduction to the study, general questions were asked regarding the mobile phone usage, whether the display setup has been used before, and how they are affiliated towards public notice areas. Thereafter, two practical tasks were carried out, each followed by a SUS questionnaire. Finally, questions were asked regarding the user's opinion on public notice areas. 


	% Looking Glass (MÃ¼ller) - 2012
	M{\"u}ller et al. \cite{Muller2012LookingGlass} present an in-depth evaluation of \textit{Looking Glass}, an interactive display setup inside of a shop window. Looking Glass gives visual feedback to passers-by and mirrored their movements. Their evaluation consisted of a pre-study, a controlled lab-study, and an ``in the wild'' field study. Both quantitative and qualitative data was collected. However, no questionnaires were used. M{\"u}ller et al. relied on observations, semi-structured interviews, and manual video recording, combined with interaction logs and a depth video log for quantitative data.

	% Other publications, semi helpful
	The publication by Beyer et al. \cite{Beyer2011} used the AttrakDiff questionnaire for evaluating user experience in their lab study. 
	% Ballagas: Sweep and Point and Shoot - 2005 - #3
	Ballagas et al. \cite{Ballagas2005} used a background, subjective and a ISO 9241-9 based questionnaire\footnote{\url{http://www.yorku.ca/mack/gi2009.html} (last accessed on April 24, 2015)}, for measuring performance.
	% Cheverst
	In the \textit{Hermes Photo Display} publication by Cheverst et al. \cite{cheverst2005hermes} a non-standardized questionnaire is used. The questionnaire is split up into four sections. The first section collected background information, ``the second section consisted of seven questions related to interface issues and general acceptability. The third section contained 14 questions related to social and community issues. Finally the fourth section contained two questions relating to possible future features.'' All questions were answered on a 5-point Likert scale. The focus was on measuring the ``notions of community''.
	A later publication by Cheverst et al. \cite{Cheverst2008} focused on determining the ``sense of community'' index. They first collected ethnographic and cultural data, followed by a focus group and a design workshop.





%%%  TEMPORARY NOTES  %%%

		% >> introduce the reader to the area of public display evaluation, how they have been evaluated, what has to be known, and so on

		% \item state how complex it is to administer / execute / conduct a survey or questionnaire
		% \item encourage the motivation for creating a plattform like this!
		% \item discrepancy between field and lab study: \cite{Ojala2011}.
		% \item field studies are much more time consuming and usually spread over a larger area to assess. Being able to automate certain parts, such as the collection of quantitative or qualitative data, will facilitate the evaluation process for researchers and give new insights for display operators.

%%%  TEMPORARY NOTES  %%%






	\subsection{Overview of Survey Platforms}
	% Other tools, possibly related to ours.
	To verify that a toolset like \textit{PDSurvey} doesn't already exist and to better understand how web-based survey platforms are designed, research was first carried out on analyzing similar approaches which already exist on the market. Already since the early days of personal computers there was an interest to conduct computer-aided surveys \cite{SnapSurveys2015AboutUs}. Snap Surveys was founded in 1981 and is one of the pioneers. The rising demand for enhanced evaluation during the new economy was met with a multitude of new web-based survey platforms. One such survey platform is SurveyMonkey, founded in 1999 and currently being on of the most popular solutions on the market for conducting web-based surveys \cite{SurveyMonkeyAboutUs}. Other well established solutions are eSurvey Creator, SoGoSurvey, and UX Suite by UsabilityTools.

	% Free solutions
	The first solution we considered was SosciSurvey \footnote{\url{https://www.soscisurvey.de/} (last accessed on November 26, 2014)}, a popular tool developed by the Institute for Communication Science at our local university. It is well suited for surveys executed on personal computers and distributed via email, however, one major drawback for us was the difficulty of extending the platform and embedding questionnaires on mobile devices and non-web-based platforms. For our purpose it was easier to build a new platform already supporting a responsive layout and RESTful interaction. Another tool, comparable to the SociSurvey platform, is LimeSurvey \footnote{\url{http://de.wikipedia.org/wiki/LimeSurvey} (last accessed on April 6, 2015)}. LimeSurvey is an open-source project based on PHP and providing an out of the box web-based survey platform.
	% Commercial solutions
	Commercial solutions considered are eSurvey Creator\footnote{\url{https://www.esurveycreator.com/} (last accessed on April 6, 2015)}, Free Online Surveys\footnote{\url{https://www.freeonlinesurveys.com/} (last accessed on April 6, 2015)}, UX Suite by UsabilityTools\footnote{\url{http://usabilitytools.com/ux-suite/} (last accessed on April 6, 2015)}, SurveyMonkey\footnote{\url{https://www.surveymonkey.com} (last accessed on April 6, 2015)}, SoGoSurvey\footnote{\url{http://www.sogosurvey.com/Features/List-of-All-Features.aspx} (last accessed on April 6, 2015)}, SurveyPlanet\footnote{\url{https://surveyplanet.com/} (last accessed on April 6, 2015)}, and Qualtrics\footnote{\url{http://www.qualtrics.com/site-intercept/} (last accessed on April 6, 2015)}. These platforms often offer a larger variety of features compared to the free solutions. For a full list of currently available web-based survey tools, refer to \cite{Capterra2015SurveyTools, Idealware2011SurveyTools}.

	Many of the reviewed solutions already support a large number of question types and provide a sophisticated administration panel. However, their main disadvantage is their lacking support of mobile phones, the missing REST API, and lack of functionality for embedding pre-configured standardized questionnaires. In addition, there are too many special requirements in the field of public display research. 
	% UX Suite
	UX Suite by UsabilityTools offers a handy backend for configuring surveys, however, it is lacking the pre-configured standardized questionnaires. 
	% Survey Monkey
	SurveyMonkey provides sample surveys\footnote{\url{https://www.surveymonkey.com/blog/en/sample-survey-questionnaire-templates/} (last accessed on April 6, 2015)} and a mobile app for conducting and evaluating surveys. Their approach already comes closer to what we are looking for. However they do not offer an API for embedding platforms with other programing languages and the standardized questionnaires are relevant to public display research.
	% TOP 3:  SoGo vs SurveyPlanet vs Qualtrics
	The best commercial solutions found on the market were SoGoSurvey, SurveyPlanet and Qualtrics. 
	% > SoGoSurvey
		SoGoSurvey offers mobile support, advanced question types (Likert scale, matrix grid, etc.), and industry-specific solutions.
	% > SurveyPlanet
		SurveyPlanet supports a comparable set of features as SoGoSurvey, additionally, also an embed code for remote embedding. This code, however, is based on iFrames, which in turn will not work for solutions with no support of web platforms.
	% > Qualtrics
		Qualtrics is another commercial solution offering a large product spectrum of surveys for website feedback. The product coming closest to our needs is `Qualtrics Site Intercept'\footnote{\url{http://www.qualtrics.com/site-intercept/} (last accessed on April 6, 2015)}, supporting a sophisticated way of embedding on websites. Sliders, feedback links, infobars and popovers are supported.



	\subsection{Distinguishing Features}

	% What makes our platform unique
	The key difference between our approach and the already existing evaluation platforms is the ability to associate each survey with the display on which the survey is carried out on, and that our platform is tailored specifically to the needs of public display evaluation. For example, some question types from traditional web-based questionnaires are not well-suited for large displays. By limiting ourselves specifically to the evaluation of public displays, we can work much more fine-grained and deliver better results. Not all question types are, for example, well suited for touch-based or gesture-based input devices. 
	Another difference is the context-based approach. By default, \textit{PDSurvey} asks the display operator to specify the context of every display connected to the platform. When enough context data is specified, this will allow for a thorough evaluation and comparison of public display installations, also considering the influence of the environment.
	% Technical differences
	Further technical differences are the ability to conduct surveys across a variety of platforms. Not only due to the responsive layout, but also due to the modular and extensible method of construction. The benefit is that the whole platform retrieves all data via a RESTful API, as of now allowing for the greatest possible coverage of end consumer devices. As a result, surveys can also be conducted on non-web-based platforms. For the currently on the market available solutions this flexibility is not yet standard.
	% Outlook
	These measures in total allow for a simplified evaluation of public displays. Additionally, a range of standardized questionnaires is supplied to facilitate the evaluation of public displays. More can be added later, based on a crowd-sourcing approach. The pre-configured standardized questionnaires will be introduced in chapter \ref{chapter:literature-evaluation}.


%%%  REMINDERS  %%%

	% The introduction should be focused on the thesis question(s). All cited work should be directly relevant to the goals of the thesis. This is not a place to summarize everything you have ever read on a subject.

	% 1. give an overview of related work
	% 2. give background information to this thesis
	% 3. describe the work of others, what have they done so far?