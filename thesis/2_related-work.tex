\section{Related Work}
\label{chapter:related-work}

	% Brief introduction
	The goal of the literature review was to find out how other researchers evaluated public displays and to develop an understanding of how these aspects can be addressed through surveys. The aim was to identify important aspects of public display deployments, from a researcher's as well form a practitioner's perspective. A short summary of the most relevant papers are described in this chapter.


	\subsection{Evaluation of Public Displays} % Previous evaluation

	Public display evaluation has already been addressed in literature. Alt et al. \cite{Alt2012HowToEvaluate} give an overview of study types, research paradigms, and evaluation methods used for evaluating public displays. M{\"u}ller et al. \cite{muller2014mirrortouch} present with MirrorTouch a follow-up evaluation and additionally extract metrics used for quantitative field studies. According to their findings, almost exclusively descriptive field studies are used in the area of public display evaluation. For a more in-depth introduction to public displays, the doctoral thesis from Alt \cite{alt2013thesis} gives an ideal overview.
	% General introduction to Evaluation 
	For a general recap of how to best design, evaluate, and report experiments, the book by Field and Hole \cite{field2003design} was used. Kirakowski \cite{kirakowski2000questionnaireFAQ} provides a useful introduction for practitioners of what to watch out for, covering the most important aspects for conducting surveys.




	\subsection{Exemplary Papers}

	A selection of papers, which have inspired us and which themselves have a good approach towards the evaluation of public displays, are amongst others: Overcoming Assumptions by Huang et al. \cite{huang2008overcoming}, Worlds of Information by Jacucci et al. \cite{jacucci2010worldsofinformation}, and Digifieds by Alt et al. \cite{alt2011digifieds}. In the following a short overview of these and other papers given.


	% Worlds of Information - 2010
	Jacucci et al. \cite{jacucci2010worldsofinformation}, with \textit{Worlds of Information}, not only provide a superb overview of evaluation methods, but also the way they evaluate their results is exemplary. The evaluation is detailed and the questions asked in their questionnaires are stated. For evaluation they used video ethnography and a variety of questionnaires. Their focus in their paper lies on aspects of group use and user engagement, which they measure through questions adapted from Flow (GameFlow), Presence (MEC), and Intrinsic Motivation (IMI) questionnaires.

	% Overvcoming Assumptions (Huang) - 2008
	In \textit{Overcoming Assumptions} Huang et al. \cite{huang2008overcoming} analyzed how ``large ambient information displays in public settings'' (public displays) were deployed and tried to understand how such displays are used, in order to derive best practices and offer design implications. Recommendations given include elements like the position of the display, content, dynamics or how to best present the message you want to convey.
	% IM Here (Huang) - 2004
	In \textit{IM Here}, an older publication by Huang et al. \cite{Huang2004} observations, informal conversations and questionnaires are used for evaluation. The focus of their evaluation is social awareness and collaboration aspects. Questionnaires were conducted before and after the primary task.

	% UBI Hotspot (Ojala) - 2010 - #41
	Ojala et al. present an evaluation of their long-term public display deployment in downtown Oulu, referred to as \textit{UBI Hotspot}. They conducted their surveys in-situ and evaluated data from a time span of eight months. Surveys were also already embedded directly onto the UBI hotspots. They aimed to analyze the usage and user acceptance rates for their setup with questions based on Nielsen's system acceptance model. Two general demographic questions (age, gender) were asked first, followed by a random selection of 8 statements from Nielsen's system acceptance model.


	% Digifieds (Alt) - 2011
	Alt et al. \cite{alt2011digifieds} created a digital noticeboard called \textit{Digifieds} and evaluated the platform using observations, interviews and a field trial. The Digifieds platform was deployed during the UbiChallenge 2011 to an urban environment in Finland and evaluated with the help of SUS questionnaires. The questionnaire accompanied the field trial and was structured as follows: after the users got a brief introduction to the study, general questions were asked regarding the mobile phone usage, whether the display setup has been used before, and how they are affiliated towards public notice areas. Thereafter, two practical tasks were carried out, each followed by a SUS questionnaire. Finally, questions were asked regarding the user's opinion on public notice areas. 


	% Looking Glass (MÃ¼ller) - 2012
	M{\"u}ller et al. \cite{Muller2012LookingGlass} present an in-depth evaluation of \textit{Looking Glass}, an interactive display setup inside of a shop window. Looking Glass gave visual feedback to passers-by and mirrored their movements. Their evaluation consisted of a pre-study, a controlled lab-study, and an ``in the wild'' field study. Both quantitative and qualitative data was collected. However, no questionnaires were used. Mueller et al. relied on observations, semi-structured interviews, and manual video recording, combined with interaction logs and a depth video log for quantitative data.

	% Other publications, semi helpful
	The publication by Beyer et al. \cite{Beyer2011} used the AttrakDiff questionnaire for evaluating user experience in their lab study. 
	% Ballagas: Sweep and Point and Shoot - 2005 - #3
	Ballagas et al. \cite{Ballagas2005} used a background, subjective and a ISO 9241-9 based questionnaire, for measuring performance.
	% Cheverst
	In the \textit{Hermes Photo Display} publication by Cheverst et al. \cite{cheverst2005hermes} a non-standardized questionnaire is used. The questionnaire is split up into four sections. The first section collected background information, ``the second section consisted of seven questions related to interface issues and general acceptability. The third section contained 14 questions related to social and community issues. Finally the fourth section contained two questions relating to possible future features.'' All questions were answered on a 5-point Likert scale. The focus was on measuring the ``notions of community''.
	A later publication by Cheverst et al. \cite{Cheverst2008} focused on determining the ``sense of community'' index. They first collected ethnographic and cultural data, followed by a focus group and a design workshop.





%%%  TEMPORARY NOTES  %%%

		% >> introduce the reader to the area of public display evaluation, how they have been evaluated, what has to be known, and so on

		% \item state how complex it is to administer / execute / conduct a survey or questionnaire
		% \item encourage the motivation for creating a plattform like this!
		% \item discrepancy between field and lab study: \cite{Ojala2011}.
		% \item field studies are much more time consuming and usually spread over a larger area to assess. Being able to automate certain parts, such as the collection of quantitative or qualitative data, will facilitate the evaluation process for researchers and give new insights for display operators.

%%%  TEMPORARY NOTES  %%%






	\subsection{Overview of Survey Platforms}
	% Other tools, possibly related to ours.
	To verify that a toolset like \textit{PDSurvey} doesn't already exist and to better understand how web-based survey platforms are carried out, research was carried out on which similar approaches already exist on the market. 



Already since the early days of personal computers there was a demand for surveys carried out aided by computers \cite{SnapSurveys2015AboutUs}. Snap Surveys was founded in 1981 and one of the pionieers.

The rising demand for enhanced evaluation during the new economy was met with the first web-based survey 

The demand for extensive evaluation of the new economy was met with emerging online survey platforms. One such survey platform is SurveyMonkey, founded in 1999 and currently being on of the most popular solutions on the market for conducting web-based surveys [6]. Other well established solutions are eSurvey Cre- ator, SoGoSurvey, and UX Suite by UsabilityTools.



	% LMU: SosciSurvey
	The first one we looked at was SosciSurvey \footnote{\url{https://www.soscisurvey.de/} (last accessed on November 26, 2014)}, a popular tool developed by the Institute for Communication Science at our university. One major drawback of their approach was the difficulty of extending and embedding the SociSurvey platform. For our purpose it was easier to build a new platform already supporting a responsive layout and RESTful interaction. Other tools similar to the SociSurvey platform are LimeSurvey \footnote{\url{http://de.wikipedia.org/wiki/LimeSurvey} (last accessed on April 6, 2015)}, an open-source PHP project.

	% Commercial solutions
	Commercial solutions considered, which often offer a much larger variety of features compared to free to use solutions, are eSurvey Creator\footnote{\url{https://www.esurveycreator.com/} (last accessed on April 6, 2015)}, Free Online Surveys\footnote{\url{https://www.freeonlinesurveys.com/} (last accessed on April 6, 2015)}, UX Suite by UsabilityTools\footnote{\url{http://usabilitytools.com/ux-suite/} (last accessed on April 6, 2015)}, SurveyMonkey\footnote{\url{https://www.surveymonkey.com} (last accessed on April 6, 2015)}, SoGoSurvey\footnote{\url{http://www.sogosurvey.com/Features/List-of-All-Features.aspx} (last accessed on April 6, 2015)}, SurveyPlanet\footnote{\url{https://surveyplanet.com/} (last accessed on April 6, 2015)}, and Qualtrics\footnote{\url{http://www.qualtrics.com/site-intercept/} (last accessed on April 6, 2015)}. For a full list of currently available web-based survey tools, refer to \cite{Capterra2015SurveyTools, Idealware2011SurveyTools}.

	Many of these solutions already support a large number of question types and a sophisticated administration panel. However, their main disadvantage is their lacking support of mobile phones, the missing REST API, and lack of functionality for embedding pre-configured standardized questionnaires. % Like the solutions by eSurvey Creator and Free Online Surveys

	UX Suite by UsabilityTools has a handy backend for configuring surveys, although it is lacking the pre-configured standardized questionnaires. 
	SurveyMonkey provides sample surveys\footnote{\url{https://www.surveymonkey.com/blog/en/sample-survey-questionnaire-templates/} (last accessed on April 6, 2015)} and a mobile app for conducting and evaluating surveys. Their approach already comes closer to what we are looking for. However they do not offer an API for embedding platforms with other programing languages and the standardized questionnaires are relevant to public display research.

	The best commercial solutions found on the market were SoGoSurvey, SurveyPlanet and Qualtrics. 

		> SoGoSurvey. One of the best solutions!

		> SurveyPlanet () seems to be the best solution found so far! However the embed code is only based on iframes.

		> Qualtrics

		> Alternative: polleverywhere.com (only for groups up to 40 students for free)




	\subsection{Distinguishing Features}

	% What makes our platform unique
	The key difference between our approach and the already existing evaluation platforms, is the ability to associate each survey with the display on which the survey is carried out on, and that the platform is tailored specifically for public display evaluation.
	By default \textit{PDSurvey} asks the display operator to specify the context of every display connected to the platform. When enough context data is specified, this will allow for a thorough evaluation and comparison of public display installations.


		\textbf{TODO: split long sentences, write shorter and more meaningful sentences}
		% Technical differences
		Further technical differences, which are not yet standard on all of the free and commercial solutions yet, is the ability to conduct surveys across a broad number of devices and platforms, due to its modular and RESTful architecture. The benefit is that the whole platform retrieves all data via a RESTful API, as of now allowing for the greatest possible coverage of end consumer devices. Additionally we offer a range of standardized questionnaires, simplifying the evaluation of public displays. These questionnaires used for evaluation will be introduced in section \ref{section:questionnaires:categorization}.

		% REST API is missing, not designed for use on public displays. Thus no overlay and seamless integration into public display applications is possible. The only exception so far might be Qualtrics, based assessing their Site Intercept demo.

	% Limitation and optimization for Touch
	Some question types from traditional web-based questionnaires are not well-suited for large displays. By limiting ourselves specifically to the evaluation of public displays, we can work much more fine-grained and deliver better results. As an example, not all question types are well suited for touch-based or gesture-based input devices. 

	% Trend towards touch
	With the introduction of the iPhone in 2007 and the iPad in 2010 users became much more accustomed to using screens with touch support. The increasing acceptance of touch screens combined with more sophisticated touch screen technology opens the path for interactive questionnaires in public settings. 

	% Conclusion
	These measures allow for a simplified evaluation of public displays. Standardized questionnaires useful for public display evaluation are already implemented by default.





%%%  REMINDERS  %%%

	% The introduction should be focused on the thesis question(s). All cited work should be directly relevant to the goals of the thesis. This is not a place to summarize everything you have ever read on a subject.

	% 1. give an overview of related work
	% 2. give background information to this thesis
	% 3. describe the work of others, what have they done so far?