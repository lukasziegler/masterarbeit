\section{Literature Evaluation}
\label{chapter:literature-evaluation}

	Our research is based on an extensive literature review, with over 100 papers viewed. This led to the development of the public display survey platform (see chapter \ref{chapter:implementation}) and the categorization of standardized questionnaires (see section \ref{section:questionnaires:categorization}). A side effect of the literature review was, besides getting a better understanding of how public displays were evaluated, getting an overview of the questions asked to evaluate public displays and their applications. This turned out to be a quite valuable approach, since we haven't seen any compilation of questionnaires used for public display evaluation so far. Our goal was to find patterns and to build clusters of questionnaires being useful for the evaluation through automated public survey display platforms.

	% Overview of this chapter
	In the following we will first describe our methodology for gathering the information (section \ref{section:questionnaires:methodology}), followed by a categorization of standardized questionnaires (section \ref{section:questionnaires:categorization}), concluded with a summary of our findings (section \ref{section:questionnaires:findings}). The categorization of the standardized questionnaires can be found in table \ref{table:standardized-questionnaires}.



\subsection{Methodology}
\label{section:questionnaires:methodology}
% My Process: Approach for Collecting Information

	The procedure for the selection of papers to review, was as follows. As a starting point all papers form the appendix of Florian Alt's doctoral thesis \cite{alt2013thesis} were read. Afterwards interesting related work and citations were examined based on the papers from the previous step. This was supplemented with targeted research on Google Scholar and the APM Digital Library. To round off the literature review, publications of two authors who are very active in this field were reviewed. 
	%A full list of all papers reviewed can be found in Appendix \ref{appendix:papers}.

	% 1) Appendix
	The first step, analyzing all literature of Florian Alt's appendix, was fairly straight forward. All papers were read from start to finish (pages 335 to 343), in order to get a first overview of the current state of research. 
	% 2) Related Work / Citations
	The second step, pursuing related work and citations of interest, was carried out in a more subjective manner. Whenever interesting papers or projects were mentioned, the cited paper was skimmed through. 
	% 3) Google Scholar Search + ACM
	For the third step, a more strategic approach was used. Based on the insights gained from the previous steps, Google Scholar and APM was checked for literature relevant to our research question. The keywords used amongst others for the research in these online libraries were: \textit{standardized surveys for usability}, \textit{standardized surveys for user experience}, \textit{user satisfaction questionnaire}, \textit{public display evaluation}, and \textit{standardized public display evaluation}.
	% 4) Individual AUTHORS keywoard research
	The last step for collecting relevant papers consisted of profiling publications of two relevant authors in the area of public display research, namely J\"org M\"uller and Marcus Foth. The process started out by first finding a list of their publications. Since the literature review made by Florian Alt (see first step) already covered papers up to 2011, only ones published between 2012 and 2014 were viewed. 
	% procedure
	On each opened paper from this time frame a keyword search was carried out, to see whether it contained an evaluation which might be relevant for us. These keywords were: \textit{questionnaire}, \textit{survey}, \textit{question}, \textit{interview}, \textit{(field) study}, and \textit{evaluation}. If none of these words could be found, the headlines and the abstract was skimmed through. All papers containing a reference to an evaluation of public displays were saved and analyzed in more detail.
	% authors
	For J\"org M\"uller the best list of his publications were found on his personal website\footnote{\url{http://joergmueller.info/publications.html} (last accessed on November 17, 2014)}, and for Marcus Foth two websites were evaluated \footnote{\url{http://www.vrolik.de/publications/} (last accessed on November 18, 2014) and \url{http://eprints.qut.edu.au/view/person/Foth,_Marcus.html} (last accessed on November 18, 2014)}. 

	% 5) Conference Proceedings
	% didn't have time for this step yet





\subsection{Standardized Questionnaires}
\label{section:questionnaires:categorization}

	As a result of the literature evaluation process the following overview of questionnaires arose. All questionnaires found during the literature review phase were categorized into a schema, inspired by the research questions introduced in chapter 2.8.2 of Florian Alt's doctoral thesis \cite{alt2013thesis}, serving as a guideline for our classification of standardized questionnaires. Since the categories \textit{audience behavior} and \textit{user performance} can not be evaluated through questionnaires, they are not represented in the following. We extended the prior categorization with findings from the literature review phase. New categories added are \textit{usability}, \textit{context}, \textit{demographics}, and \textit{other} for miscellaneous questions.

	% During the literature review phase a comprehensive list of widely used questionnaires was assessed. 
	Other people's collections incorporated into our categorization can be found in the bibliography \cite{Lewis2013HCI, Garcia2013UXResearch, Geneve2014Wiki, Chur2014Questionnaires, wechsung2008evaluation}. 

	A full overview of all standardized questionnaires found in literature can be found in table \ref{table:standardized-questionnaires}, grouped by the following categories: user experience, usability, user acceptance, user performance, display effectiveness, privacy, social impact, context, and demographics.

	% For a list of papers using the standardized questionnaires for evaluation, refer to Appendix \ref{appendix:papers} or pages 54-55 from Alt \cite{alt2013thesis}. 


	% OTHER PUBLICATIONS
	% !!! a contradiction to what I stated in chapter 2.1 >> that no such listing exists yet (for public displays), is that true!?
		A list of other people's collections can be found in the bibliography. Lewis and Sauro \cite{Lewis2013HCI} list 19 questionnaires at the HCI conference. Garcia \cite{Garcia2013UXResearch} describes the SUMI, PSSUQ, and SUS questionnaire. The Universit{\'e} de Gen{\`e}ve \cite{Geneve2014Wiki} gives an overview of usability and user experience surveys. HTW Chur \cite{Chur2014Questionnaires} provides an overview of ISONorm 110, ISOMetrics, AttrakDiff, UEQ, QUIS, and SUMI. For further information regarding standardized usability questionnaires and evaluation methods for multimodal systems the book by Wechsung and Naumann \cite{wechsung2008evaluation} can be used. 




	% % % % % % % % % % % % % % %
	%%%    ADD TABLE HERE    %%%%
	% % % % % % % % % % % % % % %

	\label{table:standardized-questionnaires}
		% > ADD A TABLE WITH ALL OF THE QUESTIONNAIRES, A SHORT DESCRIPTION, THE DATE, THE NUMBER OF QUESTIONS, THE REFERENCE HERE

	% % % % % % % % % % % % % % %
	%%%    ADD TABLE HERE    %%%%
	% % % % % % % % % % % % % % %
	


	>>> My Categorization: \url{https://docs.google.com/document/d/1D925jJ7bmRc1EZdCTz32lmW2hniMiq7GzBWxX8rmhpE/edit} (Google Docs)







	%% User Experience  &&  Usability

		We distinguished between \textit{user experience} and \textit{usability} in our categorization, although it is hard and a controversial topic in literature \cite{bevan2009difference}
		User experience describes the overall satisfaction and experience the user has with a display. Usability can be seen as a subcategory, however one difference is that usability can be measured based on hard facts such as response time, number of clicks, number of errors and has more to do with the effectiveness and efficiency. The evaluation of both user experience and usability can be carried out through questionnaires.

			>>> Sample questionnaires used for assessing the user 	experience are .... XXX TODO XXX


	\paragraph{User Acceptance}
		\textit{User acceptance} analyzes user's motives and incentives for approaching the display. The evaluation can be carried out qualitatively (subjective feedback, focus groups) or quantitatively (questionnaires).


	\paragraph{Display Effectiveness}
		Display effectiveness evaluates the economic perspective of display efficiency. 

	\paragraph{Privacy}
		Privacy takes a look at the users privacy concerns.

	\paragraph{Social Impact}
		Social impact considers everything related to social behavior, the influence on social interaction and communities, as well as social effects.

	\paragraph{Context}

		One new category is the collection of context data, relative to the public display. On most normal studies the context doesn't change during evaluation and thus is not as important. For the evaluation of public displays, especially when multiple displays are deployed in different locations running the same application, it will become if importance to also assess the static and dynamic context of each deployed display. External influences such as the weather, time of day, special events or semester break can have an influence on the number and type of people passing by a display. Additionally static context parameters, such as the display type, display size, position on wall, the size of the room might also influence how the display setup is perceived in public. Once recorded, these static and dynamic parameters can be evaluated with knowledge discovery algorithms for big data, a whole research field for itself. 
		So far no previous works are known on this area so far, evaluating a large public display deployment through an automated online platform with the help of context-based comparison. 

	
		%% Demographics
		In most surveys \textit{demographic} background information about the participants is also of interest. This varies from general questions (gender, age, religion, education), more personal questions (relationship status, family, children, country of origin), skills (personal, professional, technical), personal beliefs, political affiliation or voluntary commitment.
		Three background questionnaires for inspiration, which we haven't used ourselves yet, but which go more in depth, are the Adult Literacy and Lifeskills Survey (ALL) \footnote{\url{http://nces.ed.gov/surveys/all/} (last accessed on April 1, 2015)}, the PIAAC Conceptual Framework of the Background Questionnaire Main Survey \footnote{\url{http://www.oecd.org/site/piaac/PIAAC(2011_11)MS_BQ_ConceptualFramework_1 Dec 2011.pdf} (last accessed on April 1, 2015)} and a Police Background Questionnaire \footnote{\url{http://www.slmpd.org/images/hr_forms/commissioned/BackgroundQuestionnaire.pdf} (last accessed on April 1, 2015)}.


		%% Miscellaneous
		\textit{Miscellaneous} contains all of the questions and questionnaires, which can not be assigned to any of the previous categories 
		Cheverst et al. \cite{cheverst2005hermes} evaluated whether there were any previous experience with Bluetooth, or recommendations for possible new features. This can be 
% TODO REwRITE BOTH SENTENCES!
		For the evaluation of the Digifieds platform Alt et al. also evaluated: ``We asked them about their mobile phone usage (e.g., how often they used it, if it had a touch screen, if they used it to surf the web, and if they had installed third party apps) and whether they had used the UbiDisplays before'' \cite{alt2011digifieds}.









\subsection{Findings}
\label{section:questionnaires:findings}

	HIER DIE ERGEBNISSE AUS DER LITERATUR ARBEIT NENNEN

	% NOT SURE ABOUT THIS ONE HERE, MAYBE COMBINE "FINDINGS" WITH 



	Findings: 
	\begin{enumerate}
	\item use both quantitative and qualitative methods for data collection (explain why this is important, teaser this as a requirement for the platform, how it could be implemented)

	\item support mutliple sections, all displayed at once or (optionally) spread across multiple users

	\item support various question types (e.g. 5-point and 7-point Likert scale, multiple choice, numeric responses, comments)

	\item support lots of end devices (restful)

	\item also support non web-based platforms, some public display applications are written in native Java, Flash or other proprietary software packages

	\item evaluate not just the application running on the display, but also the entire environment. Differences in the context of the public display often result in different perceptions and user interaction.

	\item 	One constraint of public display research represents the opportunistic nature of the setups and the discrepancy between lab studies and field studies \cite{Ojala2011}. Thus there is an additional demand for evaluating each public display setup individually and if possible directly in the field.

	\item Additionally a larger number of form factors, platforms and end devices needs to be supported, to cover the whole range of public displays being out there

	\end{enumerate}



%%% Transfer to the next chapter

	These findings bring us to the next chapter, the research platform to develop, capable of conducting all of these questionnaires.





% \subsection{Requirements}
% \label{section:questionnaires:requirements}

% 	HIER DEN TRANSFER VON DEN ERKENNTNISSEN ZU DEN ANFORDERUNGEN AN DIE PLATTFORM NENNEN

% 	When taking these findings and putting them in the context of the survey platform, which we are planning to develop in chapter \ref{chapter:implementation}, we can derive the following requirements:

% 	\begin{enumerate}
% 	\item asdf
% 	\end{enumerate}





% ALTERNATIVES FOR FIRST SENTENCE

	% While performing the literature review, not only getting a better understanding of how public displays are evaluated was a result, but also getting a better overview of the questions asked to evaluate public displays and of their applications running on them
