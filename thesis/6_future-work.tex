\section{Future Work}
\label{chapter:future-work}
% inspired by: http://publications.lib.chalmers.se/records/fulltext/131908.pdf
	
Based on the development process of the \textit{PDSurvey} platform, inspired by the literature review and survey responses, we came to the following thoughts of what else might be of interest for follow-up studies.


% SURVEY PLATFORM
For the survey platform itself we had to cut back on our goals early, due to the limited resources and development time of 2,5 months. Since it is intended for fellow students to further improve on the platform, we let out certain aspects of a classic survey platform. It would be interesting to see some of the following extensions to \textit{PDSurvey}.
% first improvement: visualization
The first and biggest need for improvement is a proper visualization of the quantitative and qualitative results, e.g. with the use of information visualization and JavaScript libraries such as D3.js\footnote{\url{http://d3js.org/} (last visited on April 13, 2015)} or Morris.js\footnote{\url{http://morrisjs.github.io/morris.js/} (last visited on April 13, 2015)}. Because this was not our main focus for this thesis, we only implemented basic logging of all results, without any automated evaluation or visualization.
% Second improvement: more logging
A second aspect for improvement would be to support more data sources. Currently `only' data from questionnaires are logged, but it could also be of interest to support the logging of video feeds, audio feeds, touch interaction (pixel coordinates), or other meta data from the display setups. For logging large amounts of data an integration with storage solutions such as Dropbox could be of interest. Based on their Dropbox API\footnote{\url{https://www.dropbox.com/developers/datastore} (last visited on April 13, 2015)} files can be stored and referenced from third-party applications.
% Third, automated evaluation
A third part for improvement could be to offer more sophisticated evaluation mechanisms. When combining more log data with better evaluation mechanisms, better insights into areas such as user performance can be established. Integrating the functionality to automatically evaluate all survey responses based on their validity, reliability and on metrics such as standard deviation, will not only simplify the overall evaluation of public displays and their interactive applications, but also the overall quality of the end product being evaluated.






%%%  NEW RESEARCH QUESTIONS  %%%

While executing our field study we thought of additional research questions, which would be of interest, but would go beyond the scope of this thesis. 
% 1st: NumQuestions PER - FEEDBACK CHANNEL 
One such aspect is the number of questions tolerated per feedback channel. While executing the field study and semi-structured interviews some people noted that they would be willing to give more detailed responses, when they could fill in the questionnaire at home. Getting better insights into the constraints of each feedback channel would be of high interest for the construction and deployment of questionnaires in public settings. To find out whether this variable differs between the chosen feedback channel, location of the display setup, and its surrounding environment, or if other factors also play a role here. Another interesting question might be in which setting a user is most willing to answer surveys on public displays. 


% 2nd: LONG questionnaires, SPLITTING UP?
Another research question of interest might be how to best deploy standardized questionnaires consisting of 20+ questions. The problem being that no user wants to complete too extensive questionnaires in a public setting. One approach could be to analyze, whether it is possible to break down long questionnaires across multiple users, and aggregate the results, taking into account that the derived findings will not be as extensive and may not allow any inferences. According to Jacucci et al. \cite{jacucci2010worldsofinformation} there are often significant similarities between standardized questionnaires. Therefore it might be possible to break down each questionnaire to its principal components, to bundle all matches, in order to reduce the total amount of questions and to be able to split all questions across multiple users on the same display. 
Should this turn out not to be feasible approach, then questionnaires of different length can still be distributed based on metrics such as user involvement, or the chosen feedback channel. Users choosing a well-established and comfortable feedback channel, such as email, might be willing to respond to longer questionnaires, than participants in public. Another approach could be to track users across time and recognize returning users, in order to continue with the questionnaire where they last left off.


% 3rd: Environment and DESIGN GUIDELINES
Getting a better understanding of public displays in general, and their design guidelines in particular, will also improve how to best integrate questionnaires in public display deployments. Finding better design guidelines for the development of interactive applications will be of benefit. Questions can include the influence of the environment, e.g. how personal questions can get in different public settings, or how much privacy the display should offer (the smaller the display, the more private the context seems). This is one assumption derived from the interview responses in our field study. The influence of the display size on parameters such as perceived privacy and security is an area of high relevance when conducting surveys in public.
Other questions of interest might be what the ideal placement of the question itself on the screen is, how to best embed the survey (as a pop up, overlay, or full screen), how (un)obtrusive the design should be, or when to best interrupt the user from his primary task (before, during, or after).

% When is the best timing for interrupting a user from his primary task? What are the chances that he will take the time to answer questionnaires in public? How can we best integrate questionnaires on and after the application itself.

% For improving how to best integrate questionnaires on public displays a better understanding which design guidelines 



% 4th: controlled LAB EXPERIMENTS >> BETTER INSIGHTS
Last but not least, getting better insights from controlled experiments in lab settings on the effects of the content, context, environment and further parameters would be interesting to find out. Being able to assess how many qualitative and/or quantitative questions can be posed, would be of interest. Also getting insights into which question types are best suited for which feedback channel. One result of our field study was the importance of fast, when possible single-click, responses.
% Concluding
All these questions would lead to getting better insights into how surveys should be constructed to take best advantage of the PDSurvey platform.


