\section{Future Work}
\label{chapter:future-work}
% inspired by: http://publications.lib.chalmers.se/records/fulltext/131908.pdf
	
Based on the development process of the \textit{PDSurvey} platform, inspired by the literature review and survey responses, we came to the following thoughts of what else might be of interest for follow-up studies.


% SURVEY PLATFORM
For the survey platform itself we had to cut back on our goals early, due to the limited resources and development time of 2,5 months. Since the intention is to further improve the platform through fellow students, certain aspects of a classic survey platform were omitted. It would be interesting to see some of the following extensions to \textit{PDSurvey}.
% first improvement: visualization
The first and biggest need for improvement is a proper visualization of the quantitative and qualitative results, e.g. with the use of information visualization and JavaScript libraries such as D3.js\footnote{\url{http://d3js.org/} (last accessed on April 13, 2015)} or Morris.js\footnote{\url{http://morrisjs.github.io/morris.js/} (last accessed on April 13, 2015)}. Because this was not our main focus for the thesis, we only implemented basic logging of all results, without any automated evaluation or visualization.
% Second improvement: more logging
A second aspect for improvement would be to support more data sources. Currently `only' data from questionnaires are logged, but it could also be of interest to support the logging of video feeds, audio feeds, touch interaction (pixel coordinates), or other meta data from the display setups. For logging large amounts of data an integration with storage solutions such as Dropbox could be of interest. Based on their Dropbox API\footnote{\url{https://www.dropbox.com/developers/datastore} (last accessed on April 13, 2015)} files can be stored and referenced from third-party applications. It is conceivable to also log audio or video responses to questionnaires.
% Third, automated evaluation
A third chance for improvement could be to offer more sophisticated evaluation mechanisms. When combining more log data with advanced evaluation mechanisms, better insights can be achieved. One such enhancement could be a context-based approach for evaluation. When collecting the context of each display setup and of every response, a comparison across a multitude of displays becomes feasible. This can provide new insights into patterns responsible for certain effects. Another improvement would be to integrate the automatic evaluation of all survey responses based on their validity, reliability and on metrics such as standard deviation. This will not only simplify the overall evaluation of public displays and their interactive applications, but also the overall quality of the end product being evaluated.
% Technical
As more information gets collected and as the platform grows, it would be good to further refine the context model and to add a proper authentication mechanism. From a technical perspective it would be good to add unit testing and to add authentication to the platform.



%%%  NEW RESEARCH QUESTIONS  %%%

While executing our field study we thought of additional research questions, which would be of interest, but would go beyond the scope of this thesis. 
% 1st: NumQuestions PER - FEEDBACK CHANNEL 
One such aspect is the number of questions tolerated per feedback channel. While executing the field study and semi-structured interviews some people noted that they would be willing to give more detailed responses, when they could fill in the questionnaire at home. Getting better insights into the constraints of each feedback channel would be of high interest for the construction and deployment of questionnaires in public settings. To find out whether this variable differs between the chosen feedback channel, location of the display setup, and its surrounding environment, or if other factors also play a role here. Another interesting question might be in which setting a user is most willing to answer surveys on public displays. 


% 2nd: LONG questionnaires, SPLITTING UP?
Another research question of interest might be how to best deploy standardized questionnaires consisting of 20+ questions. The problem is that no user wants to complete too extensive questionnaires in a public setting. One approach could be to analyze, whether it is possible to break down long questionnaires across multiple users, and aggregate the results, taking into account that the derived findings will not be as extensive and may not allow any inferences. According to Jacucci et al. \cite{jacucci2010worldsofinformation} there are often significant similarities between standardized questionnaires. Therefore it might be possible to break down each questionnaire to its principal components, to bundle all matches, in order to reduce the total amount of questions and to be able to split all questions across multiple users on the same display. 
Should this turn out not to be feasible approach, then questionnaires of different length could still be distributed based on metrics such as user involvement, or the chosen feedback channel. Users choosing a well-established and comfortable feedback channel, such as email, might be willing to respond to longer questionnaires, than participants in public. Another approach could be to track users across time and recognize returning users, in order to continue with the questionnaire where they last left off.


% 3rd: Environment and DESIGN GUIDELINES
Getting a better understanding of public displays in general, and their design guidelines in particular, will also improve how to integrate questionnaires in public display deployments in the best way possible. Finding better design guidelines for the development of interactive applications will also be of benefit for this platform. Questions can include the influence of the environment, e.g. how personal questions can get in different public settings, or how much privacy the display should offer (the smaller the display, the more private the context seems). This is one assumption derived from the interview responses in our field study. The influence of the display size on parameters such as perceived privacy and security is an area of high relevance when conducting surveys in public.
Other questions of interest might be what the ideal placement of the question itself on the screen is, how to best embed the survey (as a pop up, overlay, or full screen), how (un)obtrusive the design should be, or when to best interrupt the user from his primary task (before, during, or after).


% 4th: controlled LAB EXPERIMENTS >> BETTER INSIGHTS
Last but not least, getting better insights from through experiments in controlled lab settings on the effects of the content, context, environment and further parameters would be interesting. Being able to assess how many qualitative and/or quantitative questions can be posed, is of interest. Also getting insights into which question types are best suited for which feedback channel. One result of our field study was the importance of fast responses, when possible single-click interactions.
% Concluding
Doing further research on these questions would not just improve the \textit{PDSurvey} platform, but also lead to getting better insights into how surveys should be constructed.


%% FURTHER THOUGHTS

% Die Möglichkeit nutzen Fragebögen interaktiv zu gestalten. Ist der Nutzer zufrieden, kann man ihn nur kurz ein paar Fragen stellen und muss nicht groß in die Tiefe gehen. Ist der Nutzer verärgert, sollte man die Gelegenheit anbieten dem Möglichkeit zum Ausdruck zu geben und ihm mehr Antwortmöglichkeiten bzw. ihm eine Freitextantwort nahelegen.
